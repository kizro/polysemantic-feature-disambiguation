{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEAwvENBMEyQ"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "!pip install -q transformer-lens datasets tqdm accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import torch\n",
        "from transformer_lens import HookedTransformer\n",
        "import numpy as np\n",
        "from IPython.display import HTML, display\n",
        "import pandas as pd\n",
        "from typing import List, Tuple"
      ],
      "metadata": {
        "id": "2yx7HRbUhpaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Max Neuron Activation and Visualize\n",
        "\n",
        "model = HookedTransformer.from_pretrained(\"gelu-1l\", center_unembed=True)\n",
        "tok = model.tokenizer\n",
        "model.eval()\n",
        "LAYER = 0\n",
        "NEURON = 2029\n",
        "\n",
        "activations = {}\n",
        "def save_mlp_post(act, hook):\n",
        "    activations[\"mlp_post\"] = act.detach().cpu()\n",
        "\n",
        "model.add_hook(f\"blocks.{LAYER}.mlp.hook_post\", save_mlp_post)\n",
        "\n",
        "def get_max_neuron_activation_with_context(text: str, window: int = 3):\n",
        "\n",
        "    inputs = tok(text, return_tensors=\"pt\")\n",
        "    _ = model(inputs.input_ids)\n",
        "\n",
        "    acts = activations[\"mlp_post\"][0, :, NEURON]\n",
        "    max_val, max_idx = acts.max(dim=0)\n",
        "    max_idx = max_idx.item()\n",
        "    max_val = max_val.item()\n",
        "\n",
        "    seq_len = inputs.input_ids.size(1)\n",
        "    start = max(0, max_idx - window)\n",
        "    end   = min(seq_len, max_idx + window + 1)\n",
        "\n",
        "    context_ids = inputs.input_ids[0, start:end].tolist()\n",
        "    context_str = tok.decode(context_ids)\n",
        "\n",
        "    token_id = inputs.input_ids[0, max_idx].item()\n",
        "    token_str = tok.decode([token_id])\n",
        "\n",
        "    return {\n",
        "        \"token\": token_str,\n",
        "        \"position\": max_idx,\n",
        "        \"activation\": max_val,\n",
        "        \"context\": context_str\n",
        "    }\n",
        "\n",
        "def visualize_activations(text: str):\n",
        "\n",
        "    inputs = tok(text, return_tensors=\"pt\")\n",
        "    _ = model(inputs.input_ids)\n",
        "\n",
        "    acts = activations[\"mlp_post\"][0, :, NEURON]\n",
        "\n",
        "    min_a, max_a = acts.min(), acts.max()\n",
        "    norm_acts = (acts - min_a) / (max_a - min_a + 1e-10)\n",
        "\n",
        "    html_chunks = []\n",
        "    for idx, (token_id, score) in enumerate(zip(inputs.input_ids[0], norm_acts)):\n",
        "        token_str = tok.decode([token_id.item()])\n",
        "        color = f\"rgba(225,76,76,{score:.2f})\"\n",
        "        html_chunks.append(\n",
        "            f\"<span style='background-color:{color}; padding:2px; border-radius:3px;'>{token_str}</span>\"\n",
        "        )\n",
        "\n",
        "    html = \" \".join(html_chunks)\n",
        "    display(HTML(f\"<div style='line-height:1.6; font-size:1.1em'>{html}</div>\"))\n"
      ],
      "metadata": {
        "id": "2BVO4WtJOkAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run example text and visualize\n",
        "\n",
        "example = '''\n",
        "The kings and the queens and the princes.\n",
        "'''\n",
        "visualize_activations(example)\n",
        "res = get_max_neuron_activation_with_context(example)\n",
        "print(f\"Max‐activating token: {res['token']} (position {res['position']})\")\n",
        "print(f\"Activation value: {res['activation']:.4f}\")\n",
        "print(f\"Context window: \\\"{res['context']}\\\"\")"
      ],
      "metadata": {
        "id": "1aMFUEfqArV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print neuron 2029's top ten most boosted tokens and top ten least boosted tokens\n",
        "\n",
        "mlp    = model.blocks[LAYER].mlp\n",
        "W_out  = mlp.W_out.detach().cpu()\n",
        "W_U    = model.unembed.W_U.detach().cpu()\n",
        "\n",
        "print(\"W_out shape:\", W_out.shape)\n",
        "print(\"W_U shape:  \", W_U.shape)\n",
        "d_mlp, d_model = W_out.shape\n",
        "\n",
        "proj_vec = W_out[NEURON]\n",
        "\n",
        "logit_weights = proj_vec @ W_U\n",
        "\n",
        "topk_pos = torch.topk(logit_weights, k=10)\n",
        "topk_neg = torch.topk(logit_weights, k=10, largest=False)\n",
        "\n",
        "print(\"\\nTop 10 tokens BOOSTED by neuron\", NEURON)\n",
        "for idx, score in zip(topk_pos.indices.tolist(), topk_pos.values.tolist()):\n",
        "    token = model.tokenizer.decode([idx]).strip()\n",
        "    print(f\"  {token!r}: {score:.2f}\")\n",
        "\n",
        "print(\"\\nTop 10 tokens SUPPRESSED by neuron\", NEURON)\n",
        "for idx, score in zip(topk_neg.indices.tolist(), topk_neg.values.tolist()):\n",
        "    token = model.tokenizer.decode([idx]).strip()\n",
        "    print(f\"  {token!r}: {score:.2f}\")"
      ],
      "metadata": {
        "id": "QNowrfbXbJay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print top ten co-activiating neurons for each feature prompt\n",
        "\n",
        "def top_coactivating_neurons(text: str, top_k: int = 10):\n",
        "    toks = tok(text, return_tensors=\"pt\")\n",
        "    _ = model(toks.input_ids)\n",
        "\n",
        "    acts = activations[\"mlp_post\"][0]\n",
        "\n",
        "    peak, _ = acts.max(dim=0)\n",
        "    vals, idxs = torch.topk(peak, k=top_k)\n",
        "\n",
        "    print(f\"\\nTop {top_k} co-activating neurons for “{text[:30]}…”:\")\n",
        "    for i, v in zip(idxs.tolist(), vals.tolist()):\n",
        "        print(f\"  Neuron {i:4d}  peak act = {v:.3f}\")\n",
        "\n",
        "top_coactivating_neurons(\"the kings and three camels\")\n",
        "top_coactivating_neurons(\"for i in range(10): print(i)\")"
      ],
      "metadata": {
        "id": "kOLvgnH6Em8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top-k logits at the final position and their tokens with and without neuron 2029 ablated.\n",
        "\n",
        "test_prompt = \"The king and queen are great.\"\n",
        "TOP_K       = 10\n",
        "\n",
        "def reset():\n",
        "    model.reset_hooks()\n",
        "\n",
        "def print_topk_logits(logits, k=10):\n",
        "\n",
        "    last_logits = logits[0, -1]\n",
        "    topk = torch.topk(last_logits, k)\n",
        "\n",
        "    print(f\"Top {k} logits:\")\n",
        "\n",
        "    for idx, score in zip(topk.indices.tolist(), topk.values.tolist()):\n",
        "        token = tok.decode([idx]).strip()\n",
        "        print(f\"  {token!r}: {score:.4f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "reset()\n",
        "logits_base, _ = model.run_with_cache(test_prompt)\n",
        "print(\"=== Baseline ===\")\n",
        "print_topk_logits(logits_base, TOP_K)\n",
        "\n",
        "reset()\n",
        "def ablate_fn(acts, hook):\n",
        "    acts[..., NEURON] = 0.0\n",
        "    return acts\n",
        "\n",
        "model.add_hook(f\"blocks.{LAYER}.mlp.hook_post\", ablate_fn)\n",
        "logits_abl, _ = model.run_with_cache(test_prompt)\n",
        "print(\"=== Ablated ===\")\n",
        "print_topk_logits(logits_abl, TOP_K)"
      ],
      "metadata": {
        "id": "C1wvv1BwH-Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script for identifying and ablating cancellers as well as baseline comparison\n",
        "\n",
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME    = \"gelu-1l\"\n",
        "LAYER         = 0\n",
        "NEURON_ID     = 2029\n",
        "PROMPT_A      = \"the king and\"\n",
        "PROMPT_B      = \"for i in range\"\n",
        "TOP_CANCELERS = 5\n",
        "TOP_K_PRINT   = 10\n",
        "\n",
        "model = HookedTransformer.from_pretrained(MODEL_NAME, center_unembed=True).to(DEVICE).eval()\n",
        "tok   = model.tokenizer\n",
        "\n",
        "W_out = model.blocks[LAYER].mlp.W_out.detach()\n",
        "W_U   = model.unembed.W_U.detach()\n",
        "d_mlp, d_model = W_out.shape\n",
        "\n",
        "def run_with_cache(prompt: str):\n",
        "    return model.run_with_cache(\n",
        "        prompt,\n",
        "        names_filter=[f\"blocks.{LAYER}.mlp.hook_post\"],\n",
        "        device=DEVICE,\n",
        "    )\n",
        "\n",
        "def resid_split(prompt: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    _, cache = run_with_cache(prompt)\n",
        "    acts = cache[f\"blocks.{LAYER}.mlp.hook_post\"][0][-1]\n",
        "    v_amb = acts[NEURON_ID] * W_out[NEURON_ID]\n",
        "    rest  = (acts @ W_out) - v_amb\n",
        "    return v_amb, rest\n",
        "\n",
        "def cosine(a, b): return torch.dot(a, b).item() / (a.norm() * b.norm() + 1e-9)\n",
        "\n",
        "def print_topk_logits(vec_or_logits: torch.Tensor, k=10, note=\"\"):\n",
        "\n",
        "    if vec_or_logits.shape[0] == W_U.shape[1]:\n",
        "        logits = vec_or_logits\n",
        "    else:\n",
        "        logits = vec_or_logits @ W_U\n",
        "    topk = torch.topk(logits, k)\n",
        "    print(f\"\\nTop {k} tokens for {note}:\")\n",
        "    for idx, score in zip(topk.indices.tolist(), topk.values.tolist()):\n",
        "        token = tok.decode([idx]).strip()\n",
        "        print(f\"  {token!r:<10} {score:7.2f}\")\n",
        "\n",
        "def ablate_neurons(prompt: str, neuron_ids: List[int]) -> torch.Tensor:\n",
        "    def hook_fn(acts, hook):\n",
        "        acts[..., neuron_ids] = 0.0\n",
        "        return acts\n",
        "    return model.run_with_hooks(\n",
        "        prompt,\n",
        "        return_type=\"logits\",\n",
        "        fwd_hooks=[(f\"blocks.{LAYER}.mlp.hook_post\", hook_fn)],\n",
        "    )\n",
        "\n",
        "# 1. Identify cancellation direction\n",
        "vA, restA = resid_split(PROMPT_A)\n",
        "vB, restB = resid_split(PROMPT_B)\n",
        "\n",
        "print(\"\\n=== COSINE(v_ambiguous, rest_of_stream) ===\")\n",
        "print(f\"Prompt A ({PROMPT_A[:15]}…): {cosine(vA, restA):+.3f}\")\n",
        "print(f\"Prompt B ({PROMPT_B[:15]}…): {cosine(vB, restB):+.3f}\")\n",
        "\n",
        "# 2. Identify Cancellers for Prompt A\n",
        "_, cacheA = run_with_cache(PROMPT_A)\n",
        "actsA = cacheA[f\"blocks.{LAYER}.mlp.hook_post\"][0][-1]\n",
        "contrA = actsA[:, None] * W_out\n",
        "align_scores = torch.mv(contrA, vA) / (vA.norm() + 1e-9)\n",
        "\n",
        "cancelers = align_scores.topk(TOP_CANCELERS, largest=False).indices.tolist()\n",
        "helpers   = align_scores.topk(TOP_CANCELERS, largest=True ).indices.tolist()\n",
        "\n",
        "print(\"\\nTop canceling neurons (Prompt A):\", cancelers)\n",
        "print(\"Top reinforcing neurons (Prompt A):\", helpers)\n",
        "\n",
        "# 3. Causal test with baseline and ablated top-k\n",
        "def show_logit_change(prompt: str, neuron_group: List[int], label: str):\n",
        "    base_logits = model(prompt)\n",
        "    ablated     = ablate_neurons(prompt, neuron_group)\n",
        "\n",
        "    last_base = base_logits[0, -1]\n",
        "    last_ab   = ablated[0, -1]\n",
        "\n",
        "    target = \"]):\"\n",
        "    tgt_id = tok.encode(target)[0]\n",
        "    margin_base = last_base[tgt_id] - last_base.max()\n",
        "    margin_ab   = last_ab[tgt_id]  - last_ab.max()\n",
        "\n",
        "    print(f\"\\n→ Patch test on prompt: {prompt!r}\")\n",
        "    print(f\"  Target token: '{target}'\")\n",
        "    print(f\"  Margin baseline: {margin_base:+.3f}   after ablation ({label}): {margin_ab:+.3f}\")\n",
        "\n",
        "    print_topk_logits(last_base, TOP_K_PRINT, note=\"baseline\")\n",
        "    print_topk_logits(last_ab,   TOP_K_PRINT, note=f\"ablated {label}\")\n",
        "\n",
        "show_logit_change(PROMPT_A, cancelers, \"cancelers\")\n",
        "show_logit_change(PROMPT_B, cancelers, \"cancelers\")\n",
        "\n",
        "# 4. Display raw vector top tokens (as before)\n",
        "print_topk_logits(W_out[NEURON_ID], 20, note=\"raw W_out[2029]\")\n"
      ],
      "metadata": {
        "id": "GO6Y-aH8kxb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scipt for showing that *small, different set of neighbours cancels (or reinforces) neuron 2029 in two distinct contexts.\n",
        "\n",
        "DEVICE  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL   = \"gelu-1l\"\n",
        "LAYER   = 0\n",
        "NEURON  = 2029\n",
        "K       = 5\n",
        "PROMPT_A = \"the king and\"\n",
        "PROMPT_B = \"for i in range\"\n",
        "\n",
        "model = HookedTransformer.from_pretrained(MODEL, center_unembed=True).to(DEVICE).eval()\n",
        "tok   = model.tokenizer\n",
        "\n",
        "W_out = model.blocks[LAYER].mlp.W_out.detach()\n",
        "W_U   = model.unembed.W_U.detach()\n",
        "\n",
        "def resid_and_acts(prompt: str):\n",
        "\n",
        "    logits, cache = model.run_with_cache(\n",
        "        prompt,\n",
        "        names_filter=[f\"blocks.{LAYER}.mlp.hook_post\"],\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    acts = cache[f\"blocks.{LAYER}.mlp.hook_post\"][0][-1]\n",
        "    resid = acts @ W_out\n",
        "    return acts, resid\n",
        "\n",
        "def top_cancelers(acts, v_amb, k: int) -> List[int]:\n",
        "\n",
        "    contr = acts[:, None] * W_out\n",
        "    scores = torch.mv(contr, v_amb) / (v_amb.norm()+1e-9)\n",
        "    return scores.topk(k, largest=False).indices.tolist()\n",
        "\n",
        "def logit_margin(logits, target_id):\n",
        "    last = logits[0, -1]\n",
        "    return (last[target_id] - last.max()).item()\n",
        "\n",
        "def ablate(prompt: str, neurons: List[int]):\n",
        "    def hook_fn(acts, hook): acts[..., neurons] = 0.0; return acts\n",
        "    return model.run_with_hooks(\n",
        "        prompt,\n",
        "        return_type=\"logits\",\n",
        "        fwd_hooks=[(f\"blocks.{LAYER}.mlp.hook_post\", hook_fn)]\n",
        "    )\n",
        "\n",
        "# 1.  Get ambiguous vector once (same direction any prompt)\n",
        "acts_A, _ = resid_and_acts(PROMPT_A)\n",
        "v_amb = acts_A[NEURON] * W_out[NEURON]\n",
        "\n",
        "# 2.  Build canceler sets for each prompt\n",
        "acts_A, _ = resid_and_acts(PROMPT_A)\n",
        "acts_B, _ = resid_and_acts(PROMPT_B)\n",
        "\n",
        "cancel_A = top_cancelers(acts_A, v_amb, K)\n",
        "cancel_B = top_cancelers(acts_B, v_amb, K)\n",
        "\n",
        "\n",
        "print(f\"\\nCancelers for Prompt A: {cancel_A}\")\n",
        "print(f\"Cancelers for Prompt B: {cancel_B}\")\n",
        "print(f\"Union size = {len(set(cancel_A) | set(cancel_B))}  (should be small)\")\n",
        "\n",
        "# 3.  Measure margin changes\n",
        "def test(prompt:str, cancelers:List[int], label:str):\n",
        "    target = \"]):\"\n",
        "    tgt_id = tok.encode(target)[0]\n",
        "    base_logits = model(prompt)\n",
        "    abl_logits  = ablate(prompt, cancelers)\n",
        "    m_base = logit_margin(base_logits, tgt_id)\n",
        "    m_abl  = logit_margin(abl_logits,  tgt_id)\n",
        "    print(f\"\\nPrompt: {prompt!r}\")\n",
        "    print(f\"Using cancelers from {label}\")\n",
        "    print(f\"  logit margin baseline  : {m_base:+.3f}\")\n",
        "    print(f\"  after ablation ({label}): {m_abl:+.3f}   Δ = {m_abl-m_base:+.3f}\")\n",
        "\n",
        "print(\"\\n─── In‑context ablations ───────────────────────────\")\n",
        "test(PROMPT_A, cancel_A, \"the king and\")\n",
        "test(PROMPT_B, cancel_B, \"for i in range\")\n",
        "\n",
        "print(\"\\n─── Cross‑context ablations ─────────────────────────\")\n",
        "test(PROMPT_A, cancel_B, \"for i in range\")\n",
        "test(PROMPT_B, cancel_A, \"the king and\")\n"
      ],
      "metadata": {
        "id": "AEzZcHrK0pRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}